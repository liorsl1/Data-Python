{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit1921706bd07340c5987e763850a20d4c",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our ResNet model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "modelpath='../ResNet92PercentVer2_1.h5'\n",
    "model= load_model(modelpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the live feed , with openCV.\n",
    "### Each time it recognizes a hand-object , waits for couple of runs to check for stability , once the required stability is achieved, we send the associated frame of the video to the model to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "prev_ContursLength=0\n",
    "stable_Count = 0\n",
    "capture_Count = 0\n",
    "prev_Prediction = 0\n",
    "while camera.isOpened():\n",
    "    ret , frame=camera.read()\n",
    "    k=cv2.waitKey(10)\n",
    "    if k==32:\n",
    "        break\n",
    "    # frame of a detectable object\n",
    "    iou=frame[170:280, 170:280]\n",
    "    bound_box=cv2.rectangle(frame,(100,100),(340,340),(255,0,0),1)\n",
    "    rgbIoU=cv2.cvtColor(iou,cv2.COLOR_BGR2RGB)\n",
    "    # the captures we will feed the model with\n",
    "    snapShot=frame[101:340,101:340]\n",
    "    # skin color filtering - to detect hands better than other objects\n",
    "    lower_skin = np.array([30,20,0], dtype=np.uint8)\n",
    "    upper_skin = np.array([240,220,210], dtype=np.uint8)\n",
    "    # mask and threshold for better hand-filtering\n",
    "    maskBGR= cv2.inRange(rgbIoU,lower_skin,upper_skin)\n",
    "    thresh = cv2.threshold(maskBGR, 40, 255, cv2.THRESH_OTSU)[1]\n",
    "    thresh = cv2.erode(thresh, None, iterations=3)\n",
    "    thresh = cv2.dilate(thresh, None, iterations=3)\n",
    "    hand_conturs=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    maskCheck=np.array([255,255,255],dtype=np.uint8)\n",
    "    if len(hand_conturs)>3:\n",
    "            stable_Count+=1\n",
    "            if(stable_Count>16):\n",
    "                capture_Count+=1\n",
    "                stable_Count=0\n",
    "                prediction = predict_Gesture(snapShot,model)\n",
    "                if(prev_Prediction != prediction):\n",
    "                    prev_Prediction = prediction\n",
    "\n",
    "            cv2.putText(frame,str(prediction),(120,50), font, 2, (0,202,0), 2,cv2.LINE_AA)\n",
    "            cv2.drawContours(frame,hand_conturs,-1,(0,250,0), 1)\n",
    "            anchor_box=cv2.rectangle(frame,(170,170),(280,280),(0,255,0),1)\n",
    "            cv2.rectangle(frame,(100,100),(340,340),(0,255,0),1)\n",
    "    else:\n",
    "        #initialize stable count and prediction value\n",
    "        prediction=\"\"\n",
    "        stable_Count=0\n",
    "\n",
    "        #show the window\n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that fits the image to the model, and returns the prediction\n",
    "from keras.models import load_model\n",
    "from matplotlib.pyplot import imshow,imread\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from matplotlib.pyplot import imshow,imread\n",
    "import cv2\n",
    "def predict_Gesture(image_feed,model):\n",
    "    x = cv2.resize(image_feed,(64,64))\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x/255.0\n",
    "    prediction = model.predict(x)\n",
    "    print(prediction)\n",
    "    print(\"Highest value:\", scipy.amax(prediction),\", Classified as: \", scipy.argmax(prediction))\n",
    "    return scipy.argmax(prediction)\n"
   ]
  }
 ]
}
